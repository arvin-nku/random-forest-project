---
title: "CARTs: Greedy CART, Bagging and Random Forest"
author: "Lorenz Kiesel"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{CARTs: Greedy CART, Bagging and Random Forest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(RandomForestsPackage)
library(dplyr)
library(rlang)
library(ggplot2)
set.seed(100)
```

## Introduction
In this vignette, we will look at classification and regression trees (CARTs) as well as advanced algorithms such as bagging and random forest. These techniques are often used for machine learning, especially for classification tasks. The aim is therefore to predict the value or class as accurately as possible on the basis of observations.

## Data
The following training data is used to create a regression tree

```{r}
X1 <- runif(100, 0, 1)
X2 <- runif(100, 0, 1)  
e <- rnorm(100, 0, 0.1)
Y_reg <- X1^2 + X2 + e
data_reg_tb <- tibble(a = X1, b = X2, y = Y_reg)
data_reg_li <- list(x = matrix(c(X1, X2), nrow = 2, byrow = TRUE), y = Y_reg)
```

The following training data is used to create a classification tree

```{r}
X1 <- runif(100, 0, 1)
X2 <- runif(100, 0, 1)
e <- rnorm(100, 0, 0.1)
linear_comb <- 1.8 * X1 + 0.3 * X2 + e
Y_cla <- ifelse(linear_comb > 1, 1, 2)
data_cla_tb <- tibble(a = X1, b = X2, y = Y_cla)
data_cla_li <- list(x = matrix(c(X1, X2), nrow = 2, byrow = TRUE), y = Y_cla)
```

## Functions

There are 3 different functions for creating decision trees. All are based on the Greedy CART function:
- `greed_cart()` method creates decision trees by recursively greedy splitting the data at each node based on the criterion that maximizes information gain. The algorithm selects locally optimal decisions to create a simple tree structure that fits the training data well.
- `bagging()` is an ensemble method in which multiple models are trained on different training sets generated by bootstrapping and their predictions are combined to reduce variance and improve accuracy. It works particularly well with models such as decision trees that have a high variance.
- `random_forest()` is an extension of bagging where, in addition to the bootstrapping datasets, only a random subset of the predictors at each node of the decision tree is considered. This reduces the correlation between the trees, resulting in a more robust and accurate model.

To estimate a value or the decision rule, you can use the following function
- `prediction()`


### Functions to visualize a decision tree
-
- ???MACHEN WIR DAS NOCH ODER NICHT???
-


### Create a tree with `greedy_cart()`

`greedy_cart()` creates a decision tree using the greedy method. The first two arguments specify the names of the list elements you want to use. The `data` argument specifies the list from which the data is to be taken and `type` specifies the type for the regression case.


```{r}
data <- greedy_cart_regression(data = data_reg)
```
`data` parameter specifies the environment that contains the necessary data.
`data$tree` is the result of the decision tree generation. It saves the structure of the decision tree, including the nodes and splits that were determined during training.
`data$values` contains the actual data used to create the decision tree. These are usually the predictor and target variables.
`data$dim` indicates the dimensions of the data used.

```{r}
data$tree
data$values
data$dim
```

#### Termination conditions

If you do not specify a termination condition, you will receive a tree that contains exactly one data point in each leaf.

##### `num_leaf`

You can specify the number of leaves with `num_leaf`. Here, for example, a tree with 20 leaves is created:


```{r, eval = FALSE}
Implementierrungschech mit upadate!!! type oder greedy_cart_regression???
greedy_cart(x = x, y = y, data = data_reg, type = "reg", num_leaf = 20)
```


##### `depth`

You can use depth to specify the depth of the tree. When the specified depth is reached, the algorithm terminates. In the following example, a tree with a depth of 5 is created

```{r, eval = FALSE} 
NOTATION!!!!!
greedy_cart(x = x, y = y, data = data_reg, type = "reg", depth = 5)
```


##### `num_split`

With num_split you can specify the minimum number of training data that should be in a node, so that it is split even further. In the following example, the node is still split at 5, but no longer at 4.

```{r, eval = FALSE}
NOTAION!!!!
data_1 <- greedy_cart(x = x, y = y, data = data_reg, type = "reg", num_split = 5)
```


##### `min_num`


With `min_num` you specify how many elements are in the following sheets. If there are at least `min_num` elements in the two subsequent leaves, the node is split further. In the following example, `min_num` is initialized with 10.


```{r, eval = FALSE}
NOTATION!!!!
greedy_cart(x = x, y = y, data = data_reg, type = "reg", min_num = 10)
```


Analogous to the algorithm for the regression case in the greedy_cart method, there is the classification case. The classification method works in the same way, only one additional termination condition is added:

##### `unique`

`unique` indicates whether there is only data of one class in a node. If this is true and `unique` is therefore set to TRUE, the node is not split any further. The standard default value of `unique` is therefore FALSE.

```{r}
NOTATION!!!
greedy_cart(x = c(x1,x2), y = y, data = data_class, type = "class", unique = FALSE)
```

Abschnitt NEU CODE HIER WIRD NOCH VERÃ„NDERT!!!
### Create trees with `bagging()`

`bagging()` is an extension of greedy_cart() that uses bootstrap aggregation to create multiple training trees in order to achieve greater accuracy by aggregating their predictions. Each time the function is called,`B`different trees are created based on different bootstrap samples.


```{r}
bagging_regression <- function(X, Y, B = 100)
```




HIER NOCH DER ALGORITHMUS MIT MAJOR VOTE!!!!!



This works analogously for the classification case with:

```{r}
bagging_classification <- function(X, Y, B = 100)
```

BIS HIER!!!


### Create trees with `random_forest()`

The `random_forest()` algorithm is an extension of the bagging algorithm in which only a random subset of `m` predictors is used in each iteration step to determine the decision rules, which reduces the correlation between the trees and increases the prediction accuracy. A total of `B` trees are created, with each tree trained on `A` randomly selected data points from the original data. The termination conditions are identical to those of `greedy_cart()`, i.e. the tree construction ends as soon as the tree reaches `num_leaf`. This strategy leads to more robust models as the variance is reduced and the generalization ability is improved. In the following example, `type`= "cla", `B`= 10, `A`= 15, `m`= 1 and `num_leaf`= 20 was set.

```{r}
random_forest(x = c(X1,X2), y = Y_cla, data = data_cla_li, type = "cla", B = 10, A = 15, m = 1, num_leaf = 20, depth = NULL, num_split = 2, min_num = 1, unique = F)
```

The algorithm works in the same way for the regression case, you just have to set the `type` to "reg":

```{r}
random_forest(x = X, y = Y, data = data_reg_li, type = "reg", B = 10, A = 15, m = 1, num_leaf = 20, depth = NULL, num_split = 2, min_num = 1, unique = F)
```

### `prediction()`

`prediction()` is a function that allows predictions to be made based on a list of decision trees. The first argument is a list containing the trees in tibble form, and it must always be passed as list(), even if only one tree is used. The second argument is a matrix in which each column represents a vector of values for which predictions are to be made; the number of rows must match the x-values of the input data. The `type` argument specifies whether the trees are used for regression predictions (`type` = "reg") or classification predictions (`type` = "cla"). The function calculates a prediction for each value in the matrix by using all trees in the list and returning the average or the most frequent class of predictions, depending on the type.


```{r}
list_tree <- random_forest(x = c(X1,X2), y = Y_reg, data = data_reg_li, type = "reg", B = 5, A = 10, m = 1)
list_x <- matrix(c(0, 0.2, 0.3, 0.4, 0.6, 0.7, 0.9, 1), nrow = 2)
prediction(list_tree, list_x, type = "reg")
```


